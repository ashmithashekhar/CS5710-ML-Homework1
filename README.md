# ğŸ“˜ CS5710 Machine Learning â€” Homework 1

**University:** University of Central Missouri  
**Department:** Computer Science & Cybersecurity  
**Course:** CS5710 Machine Learning (Fall 2025)  

---

## ğŸ‘©â€ğŸ“ Student Information
- **Name:** Ashmitha Kumbham  
- **Student ID:** 700773518  
- **GitHub Repo:** [PASTE REPO URL HERE]  

---

## ğŸ“‚ Repository Structure

- `gradient_descent_linear_regression.py` â€” Source code for **Q7** (Normal Equation vs. Gradient Descent)  
- `fig_data_and_fits.png` â€” Plot of dataset with fitted regression lines  
- `fig_loss_curve.png` â€” Gradient Descent loss curve visualization  
- `requirements.txt` â€” Python dependencies  

---

## âš™ï¸ Setup & Execution

### 1. Install dependencies
```bash
pip install -r requirements.txt
2. Run the program
bash
Copy code
python gradient_descent_linear_regression.py
This will:

Print model parameters from both Normal Equation and Gradient Descent.

Save the figures (fig_data_and_fits.png, fig_loss_curve.png) in the repo directory.

ğŸ“ Assignment Questions & Solutions
Q1 â€” Function Approximation by Hand
Dataset: (1,1), (2,2), (3,2), (4,5)
Model: 
ğ‘¦
^
=
ğœƒ
0
+
ğœƒ
1
ğ‘¥
y
^
â€‹
 =Î¸ 
0
â€‹
 +Î¸ 
1
â€‹
 x

Trial (1,0) â†’ MSE = 4.5

Trial (0.5,1) â†’ MSE = 0.75

âœ… Better fit: (0.5,1) due to lower error.

Q2 â€” Random Guessing Practice
Loss function:

ğ½
(
ğœƒ
0
,
ğœƒ
1
)
=
8
(
ğœƒ
0
âˆ’
0.3
)
2
+
4
(
ğœƒ
1
âˆ’
0.7
)
2
J(Î¸ 
0
â€‹
 ,Î¸ 
1
â€‹
 )=8(Î¸ 
0
â€‹
 âˆ’0.3) 
2
 +4(Î¸ 
1
â€‹
 âˆ’0.7) 
2
 
At (0.1,0.2): 
ğ½
=
1.32
J=1.32

At (0.5,0.9): 
ğ½
=
0.48
J=0.48

âœ… Closer point: (0.5,0.9)

âš ï¸ Random guessing is inefficient â†’ does not use gradient information.

Q3 â€” First Gradient Descent Iteration
Dataset: (1,3), (2,4), (3,6), (4,5)
Start: 
ğœƒ
(
0
)
=
(
0
,
0
)
Î¸ 
(0)
 =(0,0), learning rate 
ğ›¼
=
0.01
Î±=0.01

Gradient: (-9.0, -24.5)

Update: 
ğœƒ
(
1
)
=
(
0.09
,
0.245
)
Î¸ 
(1)
 =(0.09,0.245)

ğ½
(
ğœƒ
(
0
)
)
=
21.5
J(Î¸ 
(0)
 )=21.5

ğ½
(
ğœƒ
(
1
)
)
â‰ˆ
15.256
J(Î¸ 
(1)
 )â‰ˆ15.256

âœ… Loss decreases â†’ gradient descent works.

Q4 â€” Random Guessing vs Gradient Descent
Dataset: (1,2), (2,2), (3,4), (4,6)

Random guess (0.2,0.5) â†’ 
ğ½
â‰ˆ
5.515
Jâ‰ˆ5.515

Random guess (0.9,0.1) â†’ 
ğ½
â‰ˆ
7.935
Jâ‰ˆ7.935

GD (1 step, 
ğ›¼
=
0.01
Î±=0.01) â†’ 
ğ½
â‰ˆ
10.509
Jâ‰ˆ10.509

âš ï¸ Random guess can be better at the start.
âœ… GD improves systematically after many steps.

Q5 â€” Underfitting
Symptoms: High train error + high test error.

Cause: Model too simple.

Fixes: Add complexity, improve features, reduce regularization.

Q6 â€” Comparing Models
Model A (low train error, high test error): Overfitting

Fix: Regularization, more data, simplify model, early stopping

Model B (high train & test error): Underfitting

Fix: More complexity, better features, reduce regularization

Q7 â€” Linear Regression Programming
Task: Generate dataset from 
ğ‘¦
=
3
+
4
ğ‘¥
+
ğœ–
y=3+4x+Ïµ, fit using:

Closed-form solution (Normal Equation)

Gradient Descent

Parameters:

Learning rate: 0.05

Iterations: 1000

Loss: Mean Squared Error (MSE)

Sample Output:

csharp
Copy code
[Normal Equation] intercept = 2.97, slope = 4.01
[Gradient Descent] intercept = 2.96, slope = 4.00
Final MSE (GD): 1.01
âœ… Both methods converge to nearly the same solution.

ğŸ“Š Figures
fig_data_and_fits.png â†’ Dataset + fitted lines

fig_loss_curve.png â†’ GD loss vs iterations

âœ… Notes
Implemented from scratch (no scikit-learn LinearRegression).

Both closed-form and gradient descent are coded manually.

Figures auto-generated when script runs.

ğŸ‘‰ Would you like me to also include **your Q1â€“Q6 full written answers as a separate PDF file**
