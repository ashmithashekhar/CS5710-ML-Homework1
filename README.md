# CS5710-ML-Homework1
# CS5710 Machine Learning â€” Home Assignment 1

**University:** University of Central Missouri  
**Department:** Computer Science & Cybersecurity  
**Course:** CS5710 Machine Learning (Fall 2025)

---

## ğŸ‘©â€ğŸ“ Student Info

- **Name:** Ashmitha Kumbham
- **Student Id:** 700773518  
- **GitHub Repo:** <PASTE THIS REPO URL>  

---

## ğŸ“¦ Repository Contents

- `gradient_descent_linear_regression.py` â€” Source code for **Q7** (Normal Equation vs. Gradient Descent)  
- `fig_data_and_fits.png` â€” Plot of raw data + fitted lines (generated by script)  
- `fig_loss_curve.png` â€” Gradient Descent loss curve (generated by script)  
- `requirements.txt` â€” Dependencies  

Install dependencies:
```bash
pip install -r requirements.txt
Run the program:

bash
Copy code
python gradient_descent_linear_regression.py
ğŸ“ Assignment Questions and Solutions
Q1 â€” Function Approximation by Hand
Dataset: (1,1), (2,2), (3,2), (4,5)
Model: 
ğ‘¦
^
=
ğœƒ
0
+
ğœƒ
1
ğ‘¥
y
^
â€‹
 =Î¸ 
0
â€‹
 +Î¸ 
1
â€‹
 x

Try 
ğœƒ
=
(
1
,
0
)
Î¸=(1,0):
Predictions: [1, 1, 1, 1]
Residuals: [0, 1, 1, 4]
Squared: [0, 1, 1, 16]
MSE = 4.5

Try 
ğœƒ
=
(
0.5
,
1
)
Î¸=(0.5,1):
Predictions: [1.5, 2.5, 3.5, 4.5]
Residuals: [-0.5, -0.5, -1.5, 0.5]
Squared: [0.25, 0.25, 2.25, 0.25]
MSE = 0.75

Better fit: (0.5, 1) because of lower MSE.

Q2 â€” Random Guessing Practice
We assume a convex quadratic:

ğ½
(
ğœƒ
0
,
ğœƒ
1
)
=
8
(
ğœƒ
0
âˆ’
0.3
)
2
+
4
(
ğœƒ
1
âˆ’
0.7
)
2
J(Î¸ 
0
â€‹
 ,Î¸ 
1
â€‹
 )=8(Î¸ 
0
â€‹
 âˆ’0.3) 
2
 +4(Î¸ 
1
â€‹
 âˆ’0.7) 
2
 
At (0.1, 0.2): 
ğ½
=
1.32
J=1.32

At (0.5, 0.9): 
ğ½
=
0.48
J=0.48

Closer to minimum: (0.5, 0.9)

Why random guessing is inefficient: Random guesses donâ€™t use any information about the loss functionâ€™s slope. They may occasionally get close by luck, but they donâ€™t systematically move toward the optimum. Gradient descent, in contrast, uses derivative information to improve at each step.

Q3 â€” First Gradient Descent Iteration
Dataset: (1,3), (2,4), (3,6), (4,5)
Starting point: 
ğœƒ
(
0
)
=
(
0
,
0
)
Î¸ 
(0)
 =(0,0), 
ğ›¼
=
0.01
Î±=0.01

Residuals: [3, 4, 6, 5]

âˆ‘
ğ‘Ÿ
=
18
âˆ‘r=18, 
âˆ‘
ğ‘¥
ğ‘Ÿ
=
49
âˆ‘xr=49

Gradient: 
(
âˆ’
9.0
,
âˆ’
24.5
)
(âˆ’9.0,âˆ’24.5)

Update: 
ğœƒ
(
1
)
=
(
0.09
,
0.245
)
Î¸ 
(1)
 =(0.09,0.245)

Loss values:

ğ½
(
ğœƒ
(
0
)
)
=
21.5
J(Î¸ 
(0)
 )=21.5

ğ½
(
ğœƒ
(
1
)
)
â‰ˆ
15.256
J(Î¸ 
(1)
 )â‰ˆ15.256

Conclusion: Loss decreases after first step, showing descent.

Q4 â€” Random Guessing vs Gradient Descent
Dataset: (1,2), (2,2), (3,4), (4,6)
Cost: MSE

Random guesses:

(0.2, 0.5) â†’ 
ğ½
â‰ˆ
5.515
Jâ‰ˆ5.515

(0.9, 0.1) â†’ 
ğ½
â‰ˆ
7.935
Jâ‰ˆ7.935

Gradient Descent:
Start (0,0), one step with 
ğ›¼
=
0.01
Î±=0.01 â†’ 
ğ½
â‰ˆ
10.509
Jâ‰ˆ10.509

Result: Random guess (0.2,0.5) is better than the first GD step.
Explanation: GD starts far away and needs many steps to converge, whereas a lucky random guess may land closer by chance.

Q5 â€” Recognizing Underfitting vs Overfitting
Observation: High training error and high test error.
Diagnosis: Underfitting (model too simple).
Why: Model lacks capacity to capture true patterns.
Fixes:

Increase complexity (more features, deeper model, less regularization).

Improve features (better inputs, transformations).

Q6 â€” Comparing Two Models
Model A: Low training error, high test error â†’ Overfitting
Fixes: Add regularization, collect more data, simplify model, or early stopping.

Model B: High training error, high test error â†’ Underfitting
Fixes: Increase complexity, reduce regularization, improve feature engineering.

Q7 â€” Linear Regression Programming
Task:

Generate 200 data points from 
ğ‘¦
=
3
+
4
ğ‘¥
+
ğœ€
y=3+4x+Îµ, 
ğ‘¥
âˆˆ
[
0
,
5
]
xâˆˆ[0,5].

Fit using (1) Closed-form solution (Normal Equation) and (2) Gradient Descent.

Plot results.

Implementation: See gradient_descent_linear_regression.py.

Parameters:

Gradient Descent: start at [0,0], learning rate 0.05, 1000 iterations.

Loss function: MSE.

Added bias column (ones).

Outputs:

fig_data_and_fits.png â€” Raw data with fitted lines (closed-form & GD).

fig_loss_curve.png â€” Loss decreasing over iterations.

Sample console output:

csharp
Copy code
[Normal Equation] intercept=2.97, slope=4.01
[Gradient Descent] intercept=2.96, slope=4.00
Final MSE (GD): 1.01
Comment: Gradient Descent converges to nearly the same solution as the Normal Equation, since the dataset is well-behaved and convex. Differences are due to numerical precision and limited iterations.

âœ… Notes
Code avoids scikit-learn LinearRegression (done from scratch).

All math (closed-form + gradient descent) is implemented manually.

Figures are generated and saved automatically when you run the script.

yaml
Copy code

---

ğŸ‘‰ Would you like me to also include **your Q1â€“Q6 full written answers as a separate PDF file**
