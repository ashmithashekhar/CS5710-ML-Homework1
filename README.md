# CS5710-ML-Homework1
# CS5710 Machine Learning — Home Assignment 1

**University:** University of Central Missouri  
**Department:** Computer Science & Cybersecurity  
**Course:** CS5710 Machine Learning (Fall 2025)

---

## 👩‍🎓 Student Info

- **Name:** Ashmitha Kumbham
- **Student Id:** 700773518  
- **GitHub Repo:** <PASTE THIS REPO URL>  

---

## 📦 Repository Contents

- `gradient_descent_linear_regression.py` — Source code for **Q7** (Normal Equation vs. Gradient Descent)  
- `fig_data_and_fits.png` — Plot of raw data + fitted lines (generated by script)  
- `fig_loss_curve.png` — Gradient Descent loss curve (generated by script)  
- `requirements.txt` — Dependencies  

Install dependencies:
```bash
pip install -r requirements.txt
Run the program:

bash
Copy code
python gradient_descent_linear_regression.py
📝 Assignment Questions and Solutions
Q1 — Function Approximation by Hand
Dataset: (1,1), (2,2), (3,2), (4,5)
Model: 
𝑦
^
=
𝜃
0
+
𝜃
1
𝑥
y
^
​
 =θ 
0
​
 +θ 
1
​
 x

Try 
𝜃
=
(
1
,
0
)
θ=(1,0):
Predictions: [1, 1, 1, 1]
Residuals: [0, 1, 1, 4]
Squared: [0, 1, 1, 16]
MSE = 4.5

Try 
𝜃
=
(
0.5
,
1
)
θ=(0.5,1):
Predictions: [1.5, 2.5, 3.5, 4.5]
Residuals: [-0.5, -0.5, -1.5, 0.5]
Squared: [0.25, 0.25, 2.25, 0.25]
MSE = 0.75

Better fit: (0.5, 1) because of lower MSE.

Q2 — Random Guessing Practice
We assume a convex quadratic:

𝐽
(
𝜃
0
,
𝜃
1
)
=
8
(
𝜃
0
−
0.3
)
2
+
4
(
𝜃
1
−
0.7
)
2
J(θ 
0
​
 ,θ 
1
​
 )=8(θ 
0
​
 −0.3) 
2
 +4(θ 
1
​
 −0.7) 
2
 
At (0.1, 0.2): 
𝐽
=
1.32
J=1.32

At (0.5, 0.9): 
𝐽
=
0.48
J=0.48

Closer to minimum: (0.5, 0.9)

Why random guessing is inefficient: Random guesses don’t use any information about the loss function’s slope. They may occasionally get close by luck, but they don’t systematically move toward the optimum. Gradient descent, in contrast, uses derivative information to improve at each step.

Q3 — First Gradient Descent Iteration
Dataset: (1,3), (2,4), (3,6), (4,5)
Starting point: 
𝜃
(
0
)
=
(
0
,
0
)
θ 
(0)
 =(0,0), 
𝛼
=
0.01
α=0.01

Residuals: [3, 4, 6, 5]

∑
𝑟
=
18
∑r=18, 
∑
𝑥
𝑟
=
49
∑xr=49

Gradient: 
(
−
9.0
,
−
24.5
)
(−9.0,−24.5)

Update: 
𝜃
(
1
)
=
(
0.09
,
0.245
)
θ 
(1)
 =(0.09,0.245)

Loss values:

𝐽
(
𝜃
(
0
)
)
=
21.5
J(θ 
(0)
 )=21.5

𝐽
(
𝜃
(
1
)
)
≈
15.256
J(θ 
(1)
 )≈15.256

Conclusion: Loss decreases after first step, showing descent.

Q4 — Random Guessing vs Gradient Descent
Dataset: (1,2), (2,2), (3,4), (4,6)
Cost: MSE

Random guesses:

(0.2, 0.5) → 
𝐽
≈
5.515
J≈5.515

(0.9, 0.1) → 
𝐽
≈
7.935
J≈7.935

Gradient Descent:
Start (0,0), one step with 
𝛼
=
0.01
α=0.01 → 
𝐽
≈
10.509
J≈10.509

Result: Random guess (0.2,0.5) is better than the first GD step.
Explanation: GD starts far away and needs many steps to converge, whereas a lucky random guess may land closer by chance.

Q5 — Recognizing Underfitting vs Overfitting
Observation: High training error and high test error.
Diagnosis: Underfitting (model too simple).
Why: Model lacks capacity to capture true patterns.
Fixes:

Increase complexity (more features, deeper model, less regularization).

Improve features (better inputs, transformations).

Q6 — Comparing Two Models
Model A: Low training error, high test error → Overfitting
Fixes: Add regularization, collect more data, simplify model, or early stopping.

Model B: High training error, high test error → Underfitting
Fixes: Increase complexity, reduce regularization, improve feature engineering.

Q7 — Linear Regression Programming
Task:

Generate 200 data points from 
𝑦
=
3
+
4
𝑥
+
𝜀
y=3+4x+ε, 
𝑥
∈
[
0
,
5
]
x∈[0,5].

Fit using (1) Closed-form solution (Normal Equation) and (2) Gradient Descent.

Plot results.

Implementation: See gradient_descent_linear_regression.py.

Parameters:

Gradient Descent: start at [0,0], learning rate 0.05, 1000 iterations.

Loss function: MSE.

Added bias column (ones).

Outputs:

fig_data_and_fits.png — Raw data with fitted lines (closed-form & GD).

fig_loss_curve.png — Loss decreasing over iterations.

Sample console output:

csharp
Copy code
[Normal Equation] intercept=2.97, slope=4.01
[Gradient Descent] intercept=2.96, slope=4.00
Final MSE (GD): 1.01
Comment: Gradient Descent converges to nearly the same solution as the Normal Equation, since the dataset is well-behaved and convex. Differences are due to numerical precision and limited iterations.

✅ Notes
Code avoids scikit-learn LinearRegression (done from scratch).

All math (closed-form + gradient descent) is implemented manually.

Figures are generated and saved automatically when you run the script.

yaml
Copy code

---

👉 Would you like me to also include **your Q1–Q6 full written answers as a separate PDF file**
