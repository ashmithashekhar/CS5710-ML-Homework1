# ğŸ“˜ CS5710 Machine Learning â€” Homework 1

**University:** University of Central Missouri  
**Department:** Computer Science & Cybersecurity  
**Course:** CS5710 Machine Learning (Fall 2025)  

---

## ğŸ‘©â€ğŸ“ Student Information
- **Name:** Ashmitha Kumbham  
- **Student ID:** 700773518  

---

## ğŸ“‚ Repository Structure

- `gradient_descent_linear_regression.py` â€” Source code for **Q7** (Normal Equation vs. Gradient Descent)  
- `fig_data_and_fits.png` â€” Plot of dataset with fitted regression lines  
- `fig_loss_curve.png` â€” Gradient Descent loss curve visualization  
- `requirements.txt` â€” Python dependencies  

---

## âš™ï¸ Setup & Execution

### 1. Install dependencies
```bash
pip install -r requirements.txt
2. Run the program
bash
Copy code
python gradient_descent_linear_regression.py
This will:

Print model parameters from both Normal Equation and Gradient Descent

Save the figures (fig_data_and_fits.png, fig_loss_curve.png) in the repo directory

ğŸ“ Assignment Questions & Solutions
Q1 â€” Function Approximation by Hand
Dataset: (1,1), (2,2), (3,2), (4,5)
Model: Å· = Î¸â‚€ + Î¸â‚x

Trial (Î¸â‚€, Î¸â‚) = (1, 0) â†’ MSE = 4.5

Trial (Î¸â‚€, Î¸â‚) = (0.5, 1) â†’ MSE = 0.75

âœ… Better fit: (0.5, 1) due to lower error

Q2 â€” Random Guessing Practice
Loss function:

J(Î¸â‚€, Î¸â‚) = 8(Î¸â‚€ âˆ’ 0.3)Â² + 4(Î¸â‚ âˆ’ 0.7)Â²

At (0.1, 0.2): J = 1.32

At (0.5, 0.9): J = 0.48

âœ… Closer point: (0.5, 0.9)

âš ï¸ Random guessing is inefficient â†’ does not use slope/gradient information

Q3 â€” First Gradient Descent Iteration
Dataset: (1,3), (2,4), (3,6), (4,5)
Start: Î¸â½â°â¾ = (0, 0), learning rate Î± = 0.01

Residual sum = 18, weighted residual sum = 49

Gradient = (âˆ’9.0, âˆ’24.5)

Update: Î¸â½Â¹â¾ = (0.09, 0.245)

Loss values:

J(Î¸â½â°â¾) = 21.5

J(Î¸â½Â¹â¾) â‰ˆ 15.256

âœ… Loss decreases â†’ gradient descent is working

Q4 â€” Random Guessing vs Gradient Descent
Dataset: (1,2), (2,2), (3,4), (4,6)

Random guess (0.2, 0.5) â†’ J â‰ˆ 5.515

Random guess (0.9, 0.1) â†’ J â‰ˆ 7.935

GD (1 step, Î± = 0.01) â†’ J â‰ˆ 10.509

âš ï¸ Random guess can be better at the very start
âœ… GD improves systematically with more steps

Q5 â€” Underfitting
Symptoms: High training error + high test error

Cause: Model too simple

Fixes: Increase complexity, add features, reduce regularization

Q6 â€” Comparing Models
Model A (low training error, high test error): Overfitting

Fix: Regularization, more data, simplify model, early stopping

Model B (high training & test error): Underfitting

Fix: Add complexity, improve features, reduce regularization

Q7 â€” Linear Regression Programming
Task: Generate dataset from y = 3 + 4x + Îµ, fit using:

Closed-form solution (Normal Equation)

Gradient Descent

Parameters:

Learning rate: Î± = 0.05

Iterations: 1000

Loss: Mean Squared Error (MSE)

Sample Output:

csharp
Copy code
[Normal Equation] intercept = 2.97, slope = 4.01
[Gradient Descent] intercept = 2.96, slope = 4.00
Final MSE (GD): 1.01
âœ… Both methods converge to nearly the same solution

ğŸ“Š Figures
fig_data_and_fits.png â†’ Dataset + fitted regression lines

fig_loss_curve.png â†’ Loss vs iterations (GD)

âœ… Notes
Implemented from scratch (no scikit-learn LinearRegression)

Both closed-form and gradient descent are coded manually

Figures auto-generated when script runs

yaml
Copy code

---
